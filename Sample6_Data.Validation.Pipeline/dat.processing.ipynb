{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "132f3585-158a-4f7f-82eb-2cce68105427",
   "metadata": {},
   "source": [
    "# Automated Data Validation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0bfdd79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import chdir, getcwd, path, makedirs\n",
    "from numpy import nan\n",
    "from time import time\n",
    "from logging import info, error\n",
    "import subprocess\n",
    "from yaml import YAMLError, safe_load, dump\n",
    "import datetime\n",
    "from re import sub, escape\n",
    "from string import punctuation, ascii_letters\n",
    "from random import random, choice, randint, uniform\n",
    "from psutil import virtual_memory\n",
    "from pandas import DataFrame, read_csv\n",
    "from dask import dataframe as dd\n",
    "import ray\n",
    "from modin import pandas as mpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71c5f35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\Repositories\\Professional.Portfolio\\Sample6_Data.Validation.Pipeline\n",
      "C:\\Users\\andre\\Repositories\\Professional.Portfolio\\Sample6_Data.Validation.Pipeline\n"
     ]
    }
   ],
   "source": [
    "cwd = getcwd()\n",
    "print(cwd)\n",
    "chdir('C:/Users/andre/Repositories/Professional.Portfolio/Sample6_Data.Validation.Pipeline')\n",
    "print(cwd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dd5ec7",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0caf51d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to keep track of df objects in memory\n",
    "def dfs():\n",
    "    \"\"\"\n",
    "    List all Pandas DataFrame objects currently in memory.\n",
    "\n",
    "    This function lists all Pandas DataFrame objects present in the global namespace.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of DataFrame objects.\n",
    "    \"\"\"\n",
    "    dataframes = [var for var in globals() if isinstance(globals()[var], DataFrame)]\n",
    "    print(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27879465-c30b-4fce-88df-6504a8d551be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "dfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6083ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_ram():\n",
    "    \"\"\"\n",
    "    Get information about the system's RAM (Random Access Memory) usage.\n",
    "\n",
    "    Returns:\n",
    "        psutil._common.svmem: A named tuple representing RAM usage statistics.\n",
    "    \"\"\"\n",
    "    memory_usage = virtual_memory()\n",
    "    return memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0f9cd62-2ec1-4d35-a16b-b04cd96dfbcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svmem(total=34033319936, available=27926564864, percent=17.9, used=6106755072, free=27926564864)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor_ram()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ea2783-4e70-46f2-bd52-5f7a2b0385f0",
   "metadata": {},
   "source": [
    "## Functions to Create Mock Datasets >2.0Gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "263eecbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genermate_random_value():\n",
    "    \"\"\"\n",
    "    Generate a random value with various data types and potential missing values.\n",
    "\n",
    "    Returns:\n",
    "        str, int, float, or None: A randomly generated value, which can be a string, integer, float,\n",
    "        or None (representing a missing value).\n",
    "\n",
    "    Description:\n",
    "        This function generates random values with different data types and the possibility of missing values.\n",
    "        - 10% chance of returning a missing value (None).\n",
    "        - 20% chance of returning a string consisting of 5 random special characters.\n",
    "        - 50% chance of returning a string consisting of 10 random alphanumeric characters.\n",
    "        - 20% chance of returning either a random integer between 1 and 1000 (inclusive) or\n",
    "          a random float between 0.1 and 1000.0 (inclusive).\n",
    "\n",
    "    Example:\n",
    "        Possible outputs:\n",
    "        - 'ABc!@#' (string with special characters)\n",
    "        - 123 (integer)\n",
    "        - 456.789 (float)\n",
    "        - None (missing value)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 10% chance of missing value\n",
    "    if random() < 0.1:  \n",
    "        return nan\n",
    "    # 20% chance of special character\n",
    "    elif random() < 0.2:  \n",
    "        return ''.join(choice(punctuation) for _ in range(5))\n",
    "    # 50% chance of string\n",
    "    elif random() < 0.5:  \n",
    "        return ''.join(choice(ascii_letters) for _ in range(10))\n",
    "     # 20% chance of number (integer or float)\n",
    "    else: \n",
    "        return choice([randint(1, 1000), uniform(0.1, 1000.0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cac3359b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_large_dataframe(file_type, postfix, delimiter=None):\n",
    "    \"\"\"\n",
    "    Generate a large Pandas DataFrame with random data and save it to a CSV file when it exceeds 2GB in size.\n",
    "\n",
    "    Description:\n",
    "        This function generates random data and creates a Pandas DataFrame. It keeps adding rows to the DataFrame\n",
    "        until its size exceeds 2GB. Once the size limit is reached, the DataFrame is saved to a CSV file\n",
    "        with a filename indicating the size of the dataset in millions of rows (e.g., 'mock_dataset_6M.csv').\n",
    "\n",
    "    Note:\n",
    "        The function uses the `generate_random_value` function to create random data.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    start_time0 = time()\n",
    "    data = {}\n",
    "    num_columns = 25\n",
    "    num_rows = 6000000\n",
    "\n",
    "    while True:\n",
    "        # Generate dict substructure for a Pandas dataframe\n",
    "        for i in range(num_columns):\n",
    "            column_name = f'column_{i+1}'\n",
    "            data[column_name] = [generate_random_value() for _ in range(num_rows)]\n",
    "\n",
    "        # Create a Pandas DataFrame\n",
    "        df = DataFrame(data)\n",
    "        df_size_bytes = df.memory_usage(index=True).sum()\n",
    "\n",
    "        # Check size of dataframe: if > than 2GB, save the file\n",
    "        if df_size_bytes > 2 * 1073741824: \n",
    "            filename = f'mock_dataset_{num_rows//1000000}M{postfix}.{file_type}'\n",
    "            if file_type == 'psv':\n",
    "                df.to_csv(filename, index=False, sep=delimiter)\n",
    "            elif file_type == 'csv':\n",
    "                df.to_csv(filename, index=False, sep=delimiter)\n",
    "            elif file_type == 'tsv':\n",
    "                df.to_csv(filename, index=False, sep=delimiter)\n",
    "            print(f\"Saved DataFrame to {filename}\")\n",
    "            break  \n",
    "        # Else, add another 2000000 rows to the df\n",
    "        else:\n",
    "            del df \n",
    "            num_rows += 2000000\n",
    "\n",
    "    end_time0 = time() - start_time0\n",
    "    print(end_time0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb2dc991-f6b3-4ab0-9362-8847b9083b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svmem(total=34033319936, available=27896766464, percent=18.0, used=6136553472, free=27896766464)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor_ram()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59a3e3e-0e2a-4589-b36a-2a206b55dab7",
   "metadata": {},
   "source": [
    "## Timing Functions for Pandas, Modin & Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4aa5be45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## > Pandas\n",
    "\n",
    "def load_csv_with_pandas(file_path):\n",
    "    \"\"\"\n",
    "    Load a CSV file using Pandas and measure the loading time.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        float: The time taken to load the CSV file using Pandas (in seconds).\n",
    "    \"\"\"\n",
    "    start_time1 = time()\n",
    "    df = read_csv(file_path)\n",
    "    end_time1 = time() - start_time1\n",
    "    print(end_time1, ' seconds')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c17183a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## > Modin\n",
    "\n",
    "def load_csv_with_modin(file_path):\n",
    "    \"\"\"\n",
    "    Load a CSV file using Modin and measure the loading time.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        float: The time taken to load the CSV file using Modin (in seconds).\n",
    "    \"\"\"\n",
    "    ray.init(runtime_env={'env_vars': {'__MODIN_AUTOIMPORT_PANDAS__': '1'}})\n",
    "    start_time3 = time()\n",
    "    mdf = mpd.read_csv(file_path)\n",
    "    end_time3 = time() - start_time3\n",
    "    print(end_time3, ' seconds')\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fbedd80-774e-4680-aa45-ea74a37d68bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## > Dask\n",
    "\n",
    "def load_csv_with_dask(file_path, delimiter):\n",
    "    \"\"\"\n",
    "    Load a CSV file using Dask and measure the loading time.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        float: The time taken to load the CSV file using Dask (in seconds).\n",
    "    \"\"\"\n",
    "    start_time2 = time()\n",
    "    ddf = dd.read_csv(file_path, delimiter=delimiter)\n",
    "    end_time2 = time() - start_time2\n",
    "    print(end_time2, ' seconds')\n",
    "    \n",
    "    return ddf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f94fd48e-9773-4cf7-b18e-6601a43de518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svmem(total=34033319936, available=27898613760, percent=18.0, used=6134706176, free=27898613760)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor_ram()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d931696",
   "metadata": {},
   "source": [
    "## Utility Functions for Automated Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "459d6c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_config_file(filepath):\n",
    "    \n",
    "    \"\"\"\n",
    "    Read and parse a YAML configuration file.\n",
    "\n",
    "    Parameters:\n",
    "        filepath (str): The path to the YAML configuration file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the parsed configuration data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Open specified filepath\n",
    "    with open(filepath, 'r') as datacreek:\n",
    "        # Try-Catch for YAMLError\n",
    "        try:\n",
    "            return safe_load(datacreek)\n",
    "        except YAMLError as exc:\n",
    "            # Logging library error sent to 'stdout'\n",
    "            error(exc)\n",
    "\n",
    "\n",
    "def replacer(string, char):\n",
    "    \"\"\"\n",
    "    Replace two or more consecutive occurrences of a character in a string with a single occurrence.\n",
    "\n",
    "    Parameters:\n",
    "        string (str): The string to be processed.\n",
    "        char (str): The character to be replaced.\n",
    "\n",
    "    Returns:\n",
    "        str: The string with consecutive occurrences replaced.\n",
    "    \"\"\"\n",
    "    # Use regular expression to replace two or more consecutive occurrences with a single occurrence\n",
    "    return sub(f'{escape(char)}+', char, string)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def col_header_val(df, columns):\n",
    "    \"\"\"\n",
    "    Validate and standardize column names in a Dask DataFrame based on a table configuration.\n",
    "\n",
    "    Parameters:\n",
    "        df (dd.DataFrame): The Dask DataFrame to be validated.\n",
    "        columns (list): The list of expected column names.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing a boolean indicating whether validation passed, and the validated Dask DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert all strings to lowercase\n",
    "    df.columns = df.columns.str.lower()\n",
    " \n",
    "    # Replace all whitespce at the start of col names\n",
    "    df.columns = df.columns.str.replace('[^\\w]', '_', regex=True)\n",
    "    \n",
    "    # Removes underscores from beginning & end of col names\n",
    "    df.columns = list(map(lambda x: x.strip('_'), list(df.columns)))\n",
    "   \n",
    "    # Replaces 2 or more consecutive underscores with single underscore\n",
    "    df.columns = list(map(lambda x: replacer(x, '_'), list(df.columns)))\n",
    "    \n",
    "    # Converts expected col_names for 'columns' to ensure case insensitivity during comaparison\n",
    "    expected_col = list(map(lambda x: x.lower(), columns))\n",
    "   \n",
    "  \n",
    "    # Ensures case insensitivity when comparing with expected col_names\n",
    "    df.columns = list(map(lambda x: x.lower(), list(df.columns)))\n",
    "    \n",
    "    # Sort the DataFrame by multiple columns\n",
    "    df = df[columns]\n",
    "\n",
    "    if len(df.columns) == len(expected_col) and list(expected_col) == list(df.columns):\n",
    "        print('Column name and column length validation passed')\n",
    "        return True, df\n",
    "\n",
    "    # If the above is false, then we check what the differences are between df.col and exp_col and print them\n",
    "    else:\n",
    "        print('Column name and column length validation failed')\n",
    "        # Uses set operations for taking the difference between df.col and exp_col\n",
    "        mismatched_columns_file = list(set(df.columns).difference(expected_col))\n",
    "        print('The following columns are not in the expected list:', mismatched_columns_file )\n",
    "        # Uses set operations to check diff between exp_col and df_col\n",
    "        missing_expected_columns = list(set(expected_col).difference(df.columns))\n",
    "        print('The following expected columns are not in the uploaded file:', missing_expected_columns)\n",
    "        # log results\n",
    "        info(f'df columns: {df.columns}')\n",
    "        info(f'expected columns: {expected_col}')\n",
    "        return False, df\n",
    "\n",
    "\n",
    "\n",
    "def generate_yaml_config(file_path, yaml_path, file_type, file_name, table_name, in_del, out_del, columns):\n",
    "    \"\"\"\n",
    "    Generate a YAML configuration file for data processing and save it.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path to the data file.\n",
    "        yaml_path (str): The path where the YAML configuration file will be saved.\n",
    "        file_type (str): The type of the data file (e.g., 'csv', 'parquet').\n",
    "        file_name (str): The name of the data file.\n",
    "        table_name (str): The name of the data processing table.\n",
    "        in_del (str): The inbound delimiter character.\n",
    "        out_del (str): The outbound delimiter character.\n",
    "        columns (list): A list of column names.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the generated file_path, yaml_path, file_name, file_type, table_name, in_del, out_del, columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a configuration dictionary\n",
    "    config_data = {\n",
    "        'file_path': file_path,\n",
    "        'yaml_path': yaml_path,\n",
    "        'file_name': file_name,\n",
    "        'file_type': file_type,\n",
    "        'table_name': table_name,\n",
    "        'inbound_delimiter': in_del,\n",
    "        'outbound_delimiter': out_del,\n",
    "        'skip_leading_rows': 1,\n",
    "        'columns': columns,\n",
    "    }\n",
    "\n",
    "    # Convert the dictionary to YAML format\n",
    "    yaml_config = dump(config_data, default_flow_style=False)\n",
    "\n",
    "    # Save the YAML configuration to a file\n",
    "    with open(yaml_path + f'{table_name}_config.yaml', 'w') as yaml_file:\n",
    "        yaml_file.write(yaml_config)\n",
    "        \n",
    "    return file_path, yaml_path, file_name, file_type, table_name, in_del, out_del, columns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_file(file_type, source_file, delimiter):\n",
    "    \"\"\"\n",
    "    Read data from a file into a Dask DataFrame based on the file type.\n",
    "\n",
    "    Parameters:\n",
    "        file_type (str): The type of the data file (e.g., 'csv', 'excel', 'parquet').\n",
    "        source_file (str): The path to the source data file.\n",
    "        delimiter (str): The delimiter used in the file.\n",
    "\n",
    "    Returns:\n",
    "        dd.DataFrame: A Dask DataFrame containing the data from the file.\n",
    "    \"\"\"\n",
    "    if file_type == 'csv':\n",
    "        return dd.read_csv(source_file, sep=delimiter)\n",
    "    elif file_type == 'tsv':\n",
    "        return dd.read_csv(source_file, sep=delimiter)\n",
    "    elif file_type == 'psv':\n",
    "        return dd.read_csv(source_file, sep=delimiter)\n",
    "    # Add more file types as needed...\n",
    "\n",
    "\n",
    "def construct_dataset_paths(file_path, file_name, file_type):\n",
    "    \"\"\"\n",
    "    Construct the path to the dataset.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path to the directory containing the dataset.\n",
    "        file_name (str): The name of the dataset file.\n",
    "        file_type (str): The type of the dataset file (e.g., 'csv', 'parquet').\n",
    "\n",
    "    Returns:\n",
    "        str: The complete path to the dataset file.\n",
    "    \"\"\"\n",
    "    # Construct the path to the dataset\n",
    "    source_file = path.join(file_path, f'{file_name}.{file_type}')\n",
    "    return source_file\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "def save_file(df, file_type, target_path, delimiter):\n",
    "    \"\"\"\n",
    "    Save a Dask DataFrame to a file with the specified file type.\n",
    "\n",
    "    Parameters:\n",
    "        df (dd.DataFrame): The Dask DataFrame to be saved.\n",
    "        file_type (str): The file type (e.g., 'csv', 'tsv', 'psv').\n",
    "        target_path (str): The path where the file should be saved.\n",
    "        delimiter (str): The delimiter character for the file.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the file was saved successfully or already exists, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if file_type == 'csv':\n",
    "            if path.exists(target_path):\n",
    "                print(f\"File already exists: {target_path}\")\n",
    "            else:\n",
    "                df.to_csv(target_path, index=False, single_file=True, sep=delimiter)\n",
    "        elif file_type == 'tsv':\n",
    "            if path.exists(target_path):\n",
    "                print(f\"File already exists: {target_path}\")\n",
    "            else:\n",
    "                df.to_csv(target_path, index=False, single_file=True, sep=delimiter)\n",
    "        elif file_type == 'psv':\n",
    "            if path.exists(target_path):\n",
    "                print(f\"File already exists: {target_path}\")\n",
    "            else:\n",
    "                df.to_csv(target_path, index=False, single_file=True, sep=delimiter)\n",
    "        # Add more file types as needed\n",
    "        else:\n",
    "            print(f\"Unsupported file type: {file_type}\")\n",
    "            return False\n",
    "\n",
    "        print(f\"File saved successfully: {target_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving file: {e}\")\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa54b490-df3f-4840-b343-a8cc59c3df4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svmem(total=34033319936, available=27896643584, percent=18.0, used=6136676352, free=27896643584)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor_ram()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0151444",
   "metadata": {},
   "source": [
    "## Generate >2.0Gb Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e03ce7-c51e-4ee5-a31c-3c1bf3ae1169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2.6GB dataset. Time to create: ~20min\n",
    "generate_and_save_large_dataframe(file_type='csv', '0')\n",
    "generate_and_save_large_dataframe(file_type='tsv', postfix='1', delimiter='\\t')\n",
    "generate_and_save_large_dataframe(file_type='psv', postfix='2', delimiter='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddfab97-047e-42cd-bb27-147563e858b1",
   "metadata": {},
   "source": [
    "## Test each Library for Speed in Loading >2.0Gb Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bb034ef-03a8-4b75-9db8-2f170ffc2f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.54285383224487  seconds\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "svmem(total=34033319936, available=27619098624, percent=18.8, used=6414221312, free=27619098624)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load 2.6Gb dataset with pandas and evaluate time taken\n",
    "load_csv_with_pandas('./mock_dataset_12M0.csv')\n",
    "display(dfs())\n",
    "display(monitor_ram())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "496c3673-4376-4315-a858-745820960088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 14:29:46,646\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.23307228088379  seconds\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "svmem(total=34033319936, available=22098743296, percent=35.1, used=11934576640, free=22098743296)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_csv_with_modin('./mock_dataset_12M0.csv')\n",
    "display(dfs())\n",
    "display(monitor_ram())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8013a5b5-4440-4619-9ff8-b4c1f0bdcf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05746150016784668  seconds\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "svmem(total=34033319936, available=22054789120, percent=35.2, used=11978530816, free=22054789120)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load files into dataframes with dask\n",
    "ddf0 = load_csv_with_dask('./mock_dataset_12M0.csv', delimiter=',')\n",
    "display(dfs())\n",
    "display(monitor_ram())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "606c0d79-547c-42d6-950c-5c802fb91dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014030933380126953  seconds\n",
      "0.01730966567993164  seconds\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "svmem(total=34033319936, available=22042451968, percent=35.2, used=11990867968, free=22042451968)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the other two mock-datasets\n",
    "ddf1 = load_csv_with_dask('./mock_dataset_12M1.tsv', delimiter='\\t')\n",
    "ddf2 = load_csv_with_dask('./mock_dataset_12M2.psv', delimiter='|')\n",
    "display(dfs())\n",
    "display(monitor_ram())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44ed0da-8afc-432f-8556-5aaed5bd384c",
   "metadata": {},
   "source": [
    "## Main Script: Automated Data Validation Pipeline: Using Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0e21c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column name and column length validation passed\n",
      "Index(['column_1', 'column_2', 'column_3', 'column_4', 'column_5', 'column_6',\n",
      "       'column_7', 'column_8', 'column_9', 'column_10', 'column_11',\n",
      "       'column_12', 'column_13', 'column_14', 'column_15', 'column_16',\n",
      "       'column_17', 'column_18', 'column_19', 'column_20', 'column_21',\n",
      "       'column_22', 'column_23', 'column_24', 'column_25'],\n",
      "      dtype='object')\n",
      "File saved successfully: ./tsv_files/cleansed_mock_dataset_12M0.tsv\n",
      "Successfully processed and saved: mock_dataset_12M0\n",
      "Column name and column length validation passed\n",
      "Index(['column_1', 'column_2', 'column_3', 'column_4', 'column_5', 'column_6',\n",
      "       'column_7', 'column_8', 'column_9', 'column_10', 'column_11',\n",
      "       'column_12', 'column_13', 'column_14', 'column_15', 'column_16',\n",
      "       'column_17', 'column_18', 'column_19', 'column_20', 'column_21',\n",
      "       'column_22', 'column_23', 'column_24', 'column_25'],\n",
      "      dtype='object')\n",
      "File saved successfully: ./psv_files/cleansed_mock_dataset_12M1.psv\n",
      "Successfully processed and saved: mock_dataset_12M1\n",
      "Column name and column length validation passed\n",
      "Index(['column_1', 'column_2', 'column_3', 'column_4', 'column_5', 'column_6',\n",
      "       'column_7', 'column_8', 'column_9', 'column_10', 'column_11',\n",
      "       'column_12', 'column_13', 'column_14', 'column_15', 'column_16',\n",
      "       'column_17', 'column_18', 'column_19', 'column_20', 'column_21',\n",
      "       'column_22', 'column_23', 'column_24', 'column_25'],\n",
      "      dtype='object')\n",
      "File saved successfully: ./csv_files/cleansed_mock_dataset_12M2.csv\n",
      "Successfully processed and saved: mock_dataset_12M2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## > Main Script\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # --- CREATE - DIRECTORIES: - FOR - CLEANSED DATASETS - AND - YAML - FILES --- #\n",
    "\n",
    "    makedirs('./csv_files')\n",
    "    makedirs('./csv_files/csv_config_files')\n",
    "    makedirs('./tsv_files')\n",
    "    makedirs('./tsv_files/tsv_config_files')\n",
    "    makedirs('./psv_files')\n",
    "    makedirs('./psv_files/psv_config_files')\n",
    "\n",
    "    \n",
    "\n",
    "    # --- CREATING - YAML - FILES --- #\n",
    "    \n",
    "    # List of file configurations\n",
    "    file_configurations = [\n",
    "    \n",
    "        {   'file_path': './tsv_files/',\n",
    "            'yaml_path': './tsv_files/tsv_config_files/',\n",
    "           \n",
    "            'file_name': 'mock_dataset_12M0',\n",
    "            'file_type': 'csv',\n",
    "            'table_name': 'table1',\n",
    "            'in_del': ',',\n",
    "            'out_del': '\\t',\n",
    "            'columns': ddf0.columns\n",
    "        },\n",
    "        {\n",
    "            'file_path': './psv_files/',\n",
    "            'yaml_path': './psv_files/psv_config_files/',\n",
    "            'file_name': 'mock_dataset_12M1',\n",
    "            'file_type': 'tsv',\n",
    "            'table_name': 'table2',\n",
    "            'in_del': '\\t',\n",
    "            'out_del': '|',\n",
    "            'columns': ddf1.columns\n",
    "        },\n",
    "        {\n",
    "            'file_path': './csv_files/',\n",
    "            'yaml_path': './csv_files/csv_config_files/',\n",
    "            'file_name': 'mock_dataset_12M2',\n",
    "            'file_type': 'psv',\n",
    "            'table_name': 'table2',\n",
    "            'in_del': '|',\n",
    "            'out_del': ',',\n",
    "            'columns': ddf2.columns\n",
    "        }\n",
    "    \n",
    "        # ... Add more configurations as needed ... #\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    # Create a loop to create each YAML file for each dataset\n",
    "    for config in file_configurations:\n",
    "        file_path, yaml_path, file_name, file_type, table_name, in_del, out_del, columns = generate_yaml_config(\n",
    "       \n",
    "\n",
    "                    file_path=config['file_path'],\n",
    "                    yaml_path=config['yaml_path'],\n",
    "                    file_name=config['file_name'],\n",
    "                    file_type=config['file_type'],\n",
    "                    table_name=config['table_name'],\n",
    "                    in_del=config['in_del'],\n",
    "                    out_del=config['out_del'],\n",
    "                    columns=config['columns']\n",
    "        )\n",
    "        \n",
    "        \n",
    "\n",
    "        # --- CONSTRUCT - PATHS - TO - DATASETS --- #\n",
    "\n",
    "        # Construct paths\n",
    "        source_file = construct_dataset_paths(file_path=file_path, file_name=file_name, file_type=file_type)\n",
    "\n",
    "       \n",
    "        # --- CLEANSING - OPERATIONS --- #\n",
    "\n",
    "        # ... Add cleansing ops as needed ... #\n",
    "        \n",
    "        # Read the dataset in to be cleansed using configurations\n",
    "        ddf = read_file(file_type=file_type, source_file='./' + file_name + f'.{file_type}', delimiter=in_del)\n",
    "        \n",
    "        # Perform column validation\n",
    "        bool, ddf = col_header_val(df=ddf, columns=columns)\n",
    "\n",
    "        # Print cols for visual verification\n",
    "        print(ddf.columns, end='\\n')\n",
    "        \n",
    "\n",
    "\n",
    "        # --- SAVING - CLEANSED - DATA - AT - SPECIFIED - PATHS --- #\n",
    "\n",
    "        if bool:\n",
    "\n",
    "            if out_del == ',': \n",
    "                # Define the target file path and file type\n",
    "                target_file_path = file_path + f'cleansed_{file_name}.csv'\n",
    "            elif out_del == '|':\n",
    "                target_file_path = file_path + f'cleansed_{file_name}.psv'\n",
    "            elif out_del == '\\t':\n",
    "                target_file_path = file_path + f'cleansed_{file_name}.tsv'\n",
    "    \n",
    "            # Save the file using the save_file function\n",
    "            save_result = save_file(df=ddf, file_type=file_type, target_path=target_file_path, delimiter=out_del)\n",
    "    \n",
    "            if save_result:\n",
    "                print(f'Successfully processed and saved: {file_name}')\n",
    "            else:\n",
    "                print(f'Error saving the file for: {file_name}')\n",
    "        else:\n",
    "            print(f'Validation failed for: {file_name}')\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89058d4f-4cdc-4709-ab82-1914a8591040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svmem(total=34033319936, available=13152710656, percent=61.4, used=20880609280, free=13152710656)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor_ram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1687db85-09df-4ac5-b2de-c55ea3e1c870",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
